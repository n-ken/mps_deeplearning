{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動機\n",
    "\n",
    "- パラメーターを更新するためには、誤差関数$E(x)$について、各パラメーターにおける勾配を求める必要がある。\n",
    "- 多層パーセプトロンは深いネットワーク構造であり、各パラメーターの偏微分を計算するためには深い入れ子構造の方程式を偏微分する必要があり、計算量が大きくなる。\n",
    "- 誤差逆伝播法(Back Propagation)は、勾配を効率的に求めるためのストラテジーである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "表記がややこしいと思う場合、ここは飛ばして適宜参照すると良い。\n",
    "\n",
    "### レイヤー\n",
    "\n",
    "$l = 0, \\cdots, L$。ただし、$l=0$は入力を意味する。データがどのレイヤーに属しているかは、上付き添字で示す。   \n",
    "\n",
    "### 入力、重み行列、出力\n",
    "\n",
    "$l-1$層のユニット数を$I$、$l$層のユニット数を$J$とする。第$l-1$層からの入力を次のように定義する。\n",
    "\n",
    "**入力**:   \n",
    "\n",
    "$\\mathbf{z}^{(l-1)} = \\begin{pmatrix}z_1^{(l-1)}\\\\\\vdots\\\\z_I^{(l-1)}\\end{pmatrix}$\n",
    "\n",
    "ただし、$l-1=0$の時は$\\mathbf{z}^{(0)} = \\mathbf{x}$とする。\n",
    "第$l-1$層の入力から第$l$層への出力の計算に用いる重み行列は次のように定義する。\n",
    "\n",
    "\n",
    "**重み行列**:  \n",
    "\n",
    "$\\mathbf{W}^{(l)} = \\begin{pmatrix}w_{00}^{(l)}&\\cdots&w_{0i}^{(l)}&\\cdots&w_{0I}^{(l)}\\\\w_{10}^{(l)}&\\cdots&w_{1i}^{(l)}&\\cdots&w_{1I}^{(l)}\\\\\\vdots&\\ddots&\\vdots&\\ddots&\\vdots\\\\w_{J0}^{(l)}&\\cdots&w_{Ji}^{(l)}&\\cdots&w_{JI}^{(l)}\\end{pmatrix}$\n",
    "\n",
    "ここで$w_{ij}^{(l)}$は、出力$z_{j}^{(l)}$を計算する際に使用する、第$l-1$層の入力$z_{i}^{(l-1)}$にかかる重みである。\n",
    "\n",
    "また、バイアス項を次のように定義する。\n",
    "$\\mathbf{b}^{(l)} = \\begin{pmatrix}b_0^{(l)}\\\\\\vdots\\\\b_J^{(l)}\\end{pmatrix}$\n",
    "\n",
    "**出力**:   \n",
    "\n",
    "活性化関数適用前の出力を、次のように表記する。   \n",
    "$u_j^{(l)} = \\sum_{i} w_{ji}^{(l)}z_{i}^{(l)} + b_j^{(l)}$\n",
    "\n",
    "活性化関数適用後の出力を、次のように表記する。   \n",
    "$z_j^{(l)} = f(u_j^{(l)}) = f(\\sum_{i} w_{ji}^{(l)}z_{i}^{(l)} + b_j^{(l)})$\n",
    "\n",
    "**誤差関数**:   \n",
    "\n",
    "誤差関数は次のように表記する。   \n",
    "$E(\\mathbf{z}^{(L)}, \\mathbf{y})$\n",
    "\n",
    "なお、表記が複雑になる場合は$E$と略記する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与えられたデータとパラメーターをもとに、入力から出力を計算する過程を**Feedforward Propagation**と呼ぶ。\n",
    "ある1件のデータについての第$l-1$層の入力から第$l$層の出力を計算は、次のように表現できる。\n",
    "\n",
    "$$\\mathbf{z}^{(l)} = f(\\mathbf{u}^{(l)}) = f(\\mathbf{W}^{(l)}\\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l)})$$\n",
    "\n",
    "例えば、次のようなネットワークの行列計算を考える。\n",
    "\n",
    "![](figure/feedforward1.png)\n",
    "\n",
    "行列演算は、下図のようにイメージすると良い。\n",
    "\n",
    "![](figure/feedforward2.png)\n",
    "\n",
    "各層について、(1)で計算を行うことで、ネットワークの出力$\\mathbf{z}^{(L)}$を得る。\n",
    "ネットワークの出力と正解ラベル$\\mathbf{y}$を用いて、誤差を評価できる。($E(\\mathbf{z}^{(L)}, \\mathbf{y})$を用いる。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "誤差関数$E(\\mathbf{z}^{(L)}, \\mathbf{y})$の各パラメーターについての勾配を求める。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要点\n",
    "\n",
    "Back Propagationの基本的な戦略は、求めたい偏微分$\\partial E(\\mathbf{z}^{(L)}, \\mathbf{y})$を次のように展開することである。\n",
    "\n",
    "$$\\frac{\\partial E(\\mathbf{z}^{(L)}, \\mathbf{y})}{\\partial w_{ij}^{(l)}} = \n",
    "\\frac{\\partial E(\\mathbf{z}^{(L)}, \\mathbf{y})}{\\partial u_{i}^{(l)}}\\frac{\\partial u_{i}^{(l)}}{\\partial w_{ij}^{(l)}}\\tag{1}$$\n",
    "\n",
    "このような合成関数の微分が成立する理由を考えよう。$u_{i}^{(l)}$は、次のような式で計算される。\n",
    "\n",
    "$$u_{i}^{(l)} = \\sum_{j} w_{ij}^{(l)}z_{j}^{(l-1)}$$\n",
    "\n",
    "すなわち、$w_{ij}^{(l)}$についての偏微分を求めたい場合、$w_{ij}^{(l)}$を含む関数は$u_{i}^{(l)}$のみである。したがって、上の式が成立する。(上の行列演算のイメージ図で確認してみると良い。)\n",
    "\n",
    "ここで得た第一項を次のように定義する。また、第二項は簡単に次の通りに求めることができる。\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "\\delta_{i}^{(l)} = \\frac{\\partial E(\\mathbf{z}^{(L)}, \\mathbf{y})}{\\partial u_{i}^{(l)}}\\\\\n",
    "\\frac{\\partial u_{i}^{(l)}}{\\partial w_{ij}^{(l)}} = z_{j}^{(l-1)}\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "(2)より、(1)は次のように表せる。\n",
    "\n",
    "$$\\frac{\\partial E(\\mathbf{z}^{(L)}, \\mathbf{y})}{\\partial w_{ij}^{(l)}} = \n",
    "\\delta_{i}^{(l)}z_{j}^{(l-1)}\\tag{3}$$\n",
    "\n",
    "(3)を用いて、各パラメーターの偏微分を求める手法がBack Propagationである。\n",
    "後に説明するように、$\\delta_{i}^{(l)}$は一つ上のレイヤーの$\\delta_{k}^{(l+1)}$と重みの線形和で求めることができる(Feedforwardの逆の流れ)。\n",
    "\n",
    "### 出力層\n",
    "  \n",
    "$l-1$層の入力が$i(i = 0, \\cdots, I)$、第$l$層(出力層)の出力が$j(j = 0, \\cdots, J)$だとすると、偏微分$\\partial E / \\partial w_{ji}^{(l)}a$は、(3)より次の通りとなる。\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}^{(L)}} = \n",
    "\\delta_{i}^{(L)}z_{j}^{(L-1)}$$\n",
    "\n",
    "### 中間層\n",
    "\n",
    "$l - 1$層の入力が$i(i = 0, \\cdots, I)$, 第$l$層の出力が$j(j = 0, \\cdots, J)$, 第$l+1$層の出力が$k(k = 0, \\cdots, K)$がだとする。具体的に考えるため、次のようなネットワークを用いながら説明を行う。\n",
    "\n",
    "![](figure/backprop1.png)\n",
    "\n",
    "偏微分$\\partial E / \\partial w_{ji}^{(l)}$は、(1)、(3)より次の通りである。   \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ji}^{(l)}} = \\delta_{j}^{(l)}z_{i}^{(l-1)} = \n",
    "\\frac{\\partial E}{\\partial u_{j}^{(l)}}z_{i}^{(l-1)}\\tag{4}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ここで(4)の第一項は、さらに次のように展開できる。\n",
    "\n",
    "$$\\frac{\\partial E(\\mathbf{z}^{(L)}, \\mathbf{y})}{\\partial u_{j}^{(l)}} = \n",
    "\\frac{\\partial E}{\\partial u_{0}^{(l + 1)}}\\frac{\\partial u_{0}^{(l + 1)}}{\\partial u_{j}^{(l)}} + \\cdots + \n",
    "\\frac{\\partial E}{\\partial u_{K}^{(l + 1)}}\\frac{\\partial u_{K}^{(l + 1)}}{\\partial u_{j}^{(l)}} = \n",
    "\\sum_{k} \\frac{\\partial E}{\\partial u_{k}^{(l + 1)}}\\frac{\\partial u_{k}^{(l + 1)}}{\\partial u_{j}^{(l)}}\\tag{5}$$\n",
    "\n",
    "具体例で考えると次の通りである。今、$u_j^{(l)} = u_1^{(l)}$を考える。\n",
    "$u_1{(l)}$は下図の通り、それぞれの$u_k^{(l+1)}(k = 0, \\cdots, K)$の計算に使用される。\n",
    "\n",
    "![](figure/backprop2.png)\n",
    "\n",
    "![](figure/backprop3.png)\n",
    "\n",
    "したがって、合成関数の微分によって上式のように展開される。\n",
    "\n",
    "(5)の右辺の第一項は$\\delta_{k}^{(l + 1)}$となる。代入すると次の通りとなる。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial u_{j}^{(l)}} = \n",
    "\\sum_{k} \\frac{\\partial E}{\\partial u_{k}^{(l + 1)}}\\frac{\\partial u_{k}^{(l + 1)}}{\\partial u_{j}^{(l)}} = \n",
    "\\sum_{k} \\delta_{k}^{(l + 1)}f'(u_{j}^{(l)})w_{kj}^{(l+1)} = \\delta_{j}^{(l)}\\tag{6}\n",
    "$$\n",
    "\n",
    "(6)の通り、$\\delta_{j}^{(l)}$は$l+1$層の$\\delta_{k}^{(l+1)}$を用いて簡単に計算ができる。\n",
    "\n",
    "\n",
    "### まとめ\n",
    "\n",
    "#### [$\\delta_{j}^{(l)}$の求め方]\n",
    "\n",
    "$\\delta_{j}^{(l)}$は、以下の式で求めることができる。\n",
    "\n",
    "$$\\delta_{j}^{(l)} = \\sum_{k} \\delta_{k}^{(l + 1)} w_{kj}^{(l+1)}f'(u_{j}^{(l)})$$\n",
    "\n",
    "$\\delta_{j}^{(l)}$を得るためのパラメーターの関係と行列計算のイメージは以下の通り。($\\circ$は、行列の要素同士の積を意味する演算子とする。)\n",
    "\n",
    "![](figure/backprop4.png)\n",
    "\n",
    "![](figure/backprop5.png)\n",
    "\n",
    "出力層から入力層に向かってこれらの演算を行うと、すべてのユニットの$\\delta$を求めることができる。\n",
    "\n",
    "#### [損失関数の各パラメーターについての微分の求め方]\n",
    "\n",
    "損失関数の各パラメーターについての偏微分は、以下の式で求めることができる。\n",
    "\n",
    "$$\\frac{\\partial E(\\mathbf{z}^{(L)}, \\mathbf{y})}{\\partial w_{ji}^{(l)}} = \n",
    "\\delta_{j}^{(l)}z_{i}^{(l-1)}$$\n",
    "\n",
    "下図の通り、2つのユニットを結ぶ係数についての偏微分は、上の層のユニットの$\\delta_{j}^{(l)}$と下の層のユニットの出力$z_{i}^{(l-1)}$の積で簡単に求めることができる。\n",
    "\n",
    "![](figure/backprop6.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
