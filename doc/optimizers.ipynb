{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetworkにおける最適化手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### オンライン学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適化アルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最急降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新則\n",
    "\n",
    "$$\n",
    "\\theta_{t} := \\theta_{t-1} - \\eta\\nabla J(\\theta_{t-1})\n",
    "$$\n",
    "\n",
    "#### パラメーター\n",
    "\n",
    "|パラメーター|説明|推奨値|\n",
    "|:-----:|:-----:|:-----:|\n",
    "|$\\eta$|学習率|?|\n",
    "\n",
    "#### 説明\n",
    "\n",
    "標準的な最適化アルゴリズムである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モメンタム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新則\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nu_{t} &= \\gamma \\nu_{t-1} + \\eta \\nabla J(\\theta_{t-1})\\\\\n",
    "\\theta_{t} &:= \\theta_{t-1} - \\nu_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "なお、上記の更新規則は、次のように書き換えることも可能である。\n",
    "(ここで、$\\Delta_{t} = \\theta_{t} - \\theta_{t-1}$とする。)\n",
    "\n",
    "$$\n",
    "\\theta_{t} := \\theta_{t-1} + \\gamma \\Delta_{t-1} - \\eta \\nabla J(\\theta_{t-1})\n",
    "$$\n",
    "\n",
    "(証明)  \n",
    "更新式の第1式より、\n",
    "$$\n",
    "\\nu_{t} = -\\Delta_{t}\n",
    "$$\n",
    "\n",
    "となる。上式を更新式の第1式に代入し、その結果を第2式に代入すると、\n",
    "$$\n",
    "\\theta_{t} := \\theta_{t-1} - (-\\gamma \\Delta_{t-1} + \\eta \\nabla J(\\theta_{t-1}) = \\theta_{t-1} + \\gamma\\Delta_{t-1} - \\eta \\nabla J(\\theta_{t-1})\n",
    "$$\n",
    "\n",
    "\n",
    "#### パラメーター\n",
    "\n",
    "|パラメーター|説明|推奨値|\n",
    "|:-----:|:-----|:-----|\n",
    "|$\\eta$|学習率|``0.1``?|\n",
    "|$\\gamma$|モメンタム|``0.9``|\n",
    "\n",
    "#### 説明\n",
    "\n",
    "前回のパラメーター更新の方向に対する慣性を持つ。無駄に振動することがなくなり(勾配方向に加速し)、効率的な収束が期待できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新則\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nu_{t} &= \\gamma \\nu_{t-1} + \\eta\\nabla J(\\theta_{t-1} - \\gamma \\nu_{t-1})\\\\\n",
    "\\theta_{t} &:= \\theta_{t-1} - \\nu_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### パラメーター\n",
    "\n",
    "モメンタムと同様。\n",
    "\n",
    "#### 説明\n",
    "\n",
    "NAG(Nesterov accelerated gradient)は、モメンタムをやや改良している。\n",
    "具体的には、勾配の計算に$\\theta_{t-1} - \\gamma \\nu_{t-1}$を使用している。  \n",
    "これは、$t$回目の更新後の$\\theta_{t}$の勾配の近似値を計算していることとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新則\n",
    "\n",
    "まず、ステップ$t$におけるパラメーター$\\theta_{i}$に関する目的関数の勾配を$g_{t}$とし、次のように表記する。\n",
    "$$\n",
    "g_{t} = \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "次に、ステップ$t$までの勾配の累積を次の更新則で求める。(ここで、$\\circ$はアダマール積)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_{0} &= \\epsilon\\\\\n",
    "G_{t} &= G_{t-1} + g_{t}\\circ g_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Adagradの更新規則は、上記の勾配を次のように更新する。\n",
    "\n",
    "$$\n",
    "\\theta_{t} := \\theta_{t - 1} - \\frac{\\eta}{\\sqrt{G_{t}}}\\circ g_{t}\n",
    "$$\n",
    "\n",
    "すなわち、各ステップで求めた勾配の各要素の二乗の累積を分母とした係数を用いて、各パラメーターを更新していく。  \n",
    "なお、$\\epsilon$はゼロ除算を避ける平滑化項であり、大抵は$1e-8$のオーダーである。\n",
    "\n",
    "#### パラメーター\n",
    "\n",
    "|パラメーター|説明|推奨値|\n",
    "|:-----:|:-----:|:-----:|\n",
    "|$\\eta$|学習率|``0.001``?|\n",
    "|$\\epsilon$|平滑化項|$1e-8$?|\n",
    "|$G_0$|勾配の累積二乗和|$\\epsilon$|\n",
    "\n",
    "#### 説明\n",
    "\n",
    "AdaGradは、各パラメーター$\\theta_i$毎に異なる学習率を使用する。  \n",
    "勾配に乗じる分母に、各反復で求めた勾配の二乗を累積していくため、学習率は反復回数に応じて減少していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新則\n",
    "\n",
    "AdaGradのように勾配の二乗を累積させる代わりに、次のように勾配の二乗の期待値(移動平均)を定義する。\n",
    "$$\n",
    "E[g^{2}]_{t} = \\gamma E[g^{2}]_{t-1} + (1 - \\gamma)g^{2}_t\n",
    "$$\n",
    "\n",
    "また、パラメーターの変化(差分)の二乗の期待値を(移動平均)を次のように定義する。\n",
    "$$\n",
    "E[\\Delta\\theta^{2}]_{t} = \\gamma E[\\Delta\\theta^{2}]_{t-1} + (1 - \\gamma)\\Delta\\theta^{2}_{t}\n",
    "$$\n",
    "\n",
    "これらを、$G_t$および$\\eta$に代入したものが、AdaDeltaである。\n",
    "\n",
    "$$\n",
    "\\theta_{t} := \\theta_{t-1} - \\frac{\\sqrt{E[\\theta^{2}]_{t-1} + \\epsilon}}{\\sqrt{E[g^{2}]_{t} + \\epsilon}}\\circ g_{t}\n",
    "$$\n",
    "\n",
    "なお、$\\Delta\\theta_{t}$はまさに更新式の第二項であり、更新時にはこの項を用いて上記アルゴリズムを計算すれば良い。\n",
    "\n",
    "#### パラメーター\n",
    "\n",
    "|パラメーター|説明|推奨値|\n",
    "|:-----:|:-----:|:------:|\n",
    "|$\\gamma$|移動平均の重み|``0.95``?``0.9``?|\n",
    "|$E[g^{2}]_{0}$|勾配の二乗の期待値|``0``|\n",
    "|$E[\\theta^{2}]_{0}$|パラメーター変化の二乗の期待値|``0``|\n",
    "\n",
    "#### 説明\n",
    "\n",
    "AdaDeltaは、急速かつ単調な学習率の低下を防ぐためにAdaGradを改良したもの。  \n",
    "学習率を初期値に与える必要がない。\n",
    "\n",
    "なお、$\\sqrt{E[g^{2}]_{t} + \\epsilon}$は勾配の$RMS$、$\\sqrt{E[\\theta^{2}]_{t} + \\epsilon}$はパラメーター差分の$RMS$であることから、次の式のようにも表現できる。\n",
    "\n",
    "$$\n",
    "\\theta_{t} := \\theta_{t - 1} - \\frac{RMS[\\theta]_{t-1}}{RMS[g]_{t}}\\circ g_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新則\n",
    "\n",
    "AdaDeltaと同様、勾配の二乗の期待値を次のように定義する。\n",
    "\n",
    "$$\n",
    "E[g^{2}]_{t} = \\gamma E[g^{2}]_{t-1} + (1 - \\gamma)\\circ g^{2}_{t}\n",
    "$$\n",
    "\n",
    "更新式は次の通り。\n",
    "\n",
    "$$\n",
    "\\theta_{t} := \\theta_{t-1} - \\frac{\\eta}{\\sqrt{E[g^{2}]_{t} + \\epsilon}}\\circ g_{t}\n",
    "$$\n",
    "\n",
    "#### パラメーター\n",
    "\n",
    "|パラメーター|説明|推奨値|\n",
    "|:-----:|:-----:|:-----:|\n",
    "|$\\eta$|学習率|``0.001``|\n",
    "|$\\gamma$|移動平均の重み|``0.9``|\n",
    "\n",
    "#### 説明\n",
    "\n",
    "AdaGradの改良版であり、AdaDeltaを若干シンプルにしたもの。  \n",
    "Courseraの講義でHinton教授が提案した手法であり、$\\eta$が``0.001``のときに$\\gamma$は``0.9``にすることを推奨している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新則\n",
    "\n",
    "Adamは、RMSPropで用いた勾配の二乗の期待値に加え、勾配の期待値を使用する。\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "m_{t} = \\beta_{1}m_{t-1} + (1 - \\beta_{1})\\circ g_{t}\\\\\n",
    "v_{t} = \\beta_{2}v_{t-1} + (1 - \\beta_{2})\\circ g^{2}_{t}\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "求めた$m_{t}, v_{t}$のバイアスを補正する。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{m}_{t} = \\frac{m_{t}}{1 - \\beta^{t}_{1}}\\\\\n",
    "\\hat{v}_{t} = \\frac{v_{t}}{1 - \\beta^{t}_{2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "これらを用いて、次式でパラメーターの更新を行う。\n",
    "$$\n",
    "\\theta_{t} := \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_{t}} + \\epsilon}\\circ \\hat{m}_{t}\n",
    "$$\n",
    "\n",
    "なお、$t=1$からスタートする点に注意する。\n",
    "\n",
    "#### パラメーター\n",
    "\n",
    "|パラメーター|説明|推奨値|\n",
    "|:-----:|:-----:|:-----:|\n",
    "|$\\beta_{1}$|勾配の移動平均の重み|``0.9``|\n",
    "|$\\beta_{2}$|勾配の二乗の移動平均の重み|``0.999``|\n",
    "|$\\eta$|学習率|``0.001``|\n",
    "|$\\epsilon$|平滑化項|``10e-8``|\n",
    "\n",
    "#### 説明\n",
    "\n",
    "AdagradやRMSpropに勾配の期待値を追加している。  \n",
    "また、それらは勾配の一次・二次モーメントの概算値とみなすことができるため、バイアス補正を行う。\n",
    "得られた値をもとに、パラメーターを更新していく。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSPropGraves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更新則\n",
    "\n",
    "Adamと同様、勾配および勾配の二乗の移動平均を定義する。(ただし、重みは共有する)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "n_{t} &= \\gamma n_{t -1} + (1 - \\gamma) \\circ g^{2}_{t}\\\\\n",
    "m_{t} &= \\gamma m_{t-1} + (1 - \\gamma) \\circ g_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "以上を用いて、次の更新式でパラメーターを更新する。\n",
    "\n",
    "$$\n",
    "\\Delta_{t} = \\mu \\Delta_{t-1} - \\frac{\\eta}{\\sqrt{n_{t} - m^{2}_{t} + \\epsilon}}\\circ g_{t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_{t} = \\theta_{t-1} + \\Delta_{t}\n",
    "$$\n",
    "\n",
    "#### パラメーター\n",
    "\n",
    "|パラメーター|説明|推奨値|\n",
    "|:-----:|:-----:|:-----:|\n",
    "|$\\gamma$|移動平均の重み|``0.95``|\n",
    "|$\\mu$|モメンタム|``0.9``|\n",
    "|$\\eta$|学習率|``0.0001``|\n",
    "|$\\epsilon$|平滑化項|``0.0001``|\n",
    "\n",
    "\n",
    "#### 説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [勾配降下法の最適化アルゴリズムを概観する](http://postd.cc/optimizing-gradient-descent/)\n",
    "- [確率的勾配降下法 - Wikipedia](https://ja.wikipedia.org/wiki/確率的勾配降下法)\n",
    "- 岡谷貴之:「深層学習」(講談社, 2015)\n",
    "- [Source code for Chainer.optimizers.rmsprop_graves](http://docs.chainer.org/en/stable/_modules/chainer/optimizers/rmsprop_graves.html#RMSpropGraves)\n",
    "- Alex Graves, Generating Sequences With Recurrent Neural Networks, arXiv:1308.0850 [\\[Paper\\]](http://arxiv.org/abs/1308.0850)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
